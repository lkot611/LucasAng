{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section must be included at the beginning of each new notebook. Remember to change the app name. \n",
    "# If you're using VirtualBox, change the below to '/home/user/spark-2.1.1-bin-hadoop2.7'\n",
    "import findspark\n",
    "findspark.init('/home/ubuntu/spark-2.1.1-bin-hadoop2.7')\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('IterationFour').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('Dataset/San Francisco 2017.csv',header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff = spark.read.csv('Dataset/Radio_Codes.csv',header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = spark.read.csv('Dataset/final.csv',header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+----------+---------+-----------+--------+---------------+------------+--------------------+-------------+-----+---------+---------------+--------------------+\n",
      "|  CrimeId|OriginalCrimeTypName|ReportDate| CallDate|OffenseDate|CallTime|   CallDateTime| Disposition|             Address|         City|State|Agency Id|    AddressType|      CommonLocation|\n",
      "+---------+--------------------+----------+---------+-----------+--------+---------------+------------+--------------------+-------------+-----+---------+---------------+--------------------+\n",
      "|170432492|        Passing Call| 2/12/2017|2/12/2017|  2/12/2017|   17:04|2/12/2017 17:04|Not recorded|Sunnydale Av/hahn St|San Francisco|   CA|        1|   Intersection|                null|\n",
      "|170603312|     Muni Inspection|  3/1/2017| 3/1/2017|   3/1/2017|   17:37| 3/1/2017 17:37|         HAN|  16th St/mission St|San Francisco|   CA|        1|   Intersection|                null|\n",
      "|170453991|        Traffic Stop| 2/14/2017|2/14/2017|  2/14/2017|   23:01|2/14/2017 23:01|         CIT|    5th St/jessie St|San Francisco|   CA|        1|   Intersection|                null|\n",
      "|170453992|        Traffic Stop| 2/14/2017|2/14/2017|  2/14/2017|   23:01|2/14/2017 23:01|         CIT|Cambridge St/sill...|San Francisco|   CA|        1|   Intersection|                null|\n",
      "|170912137|        Passing Call|  4/1/2017| 4/1/2017|   4/1/2017|   13:46| 4/1/2017 13:46|         HAN| 0 Block Of Cargo Wy|San Francisco|   CA|        1|Common Location|             Pier 94|\n",
      "|170403723|         Return Call|  2/9/2017| 2/9/2017|   2/9/2017|   20:40| 2/9/2017 20:40|         GOA|1300 Block Of Web...|San Francisco|   CA|        1|Common Location|Safeway-Webster S...|\n",
      "|170912443|          Trespasser|  4/1/2017| 4/1/2017|   4/1/2017|   15:06| 4/1/2017 15:06|         HAN|500 Block Of Elli...|San Francisco|   CA|        1|Premise Address|                null|\n",
      "|170312929|         Petty Theft| 1/31/2017|1/31/2017|  1/31/2017|   17:39|1/31/2017 17:39|         REP|200 Block Of Hawt...|San Francisco|   CA|        1|Premise Address|                null|\n",
      "|170501376|                 Rep| 2/19/2017|2/19/2017|  2/19/2017|   11:20|2/19/2017 11:20|         REP|300 Block Of Cali...|San Francisco|   CA|        1|Premise Address|                null|\n",
      "|170800679|                 915| 3/21/2017|3/21/2017|  3/21/2017|    7:33| 3/21/2017 7:33|         HAN|300 Block Of Exec...|San Francisco|   CA|        1|Premise Address|                null|\n",
      "|170820727|              22500e| 3/23/2017|3/23/2017|  3/23/2017|    7:57| 3/23/2017 7:57|         HAN|0 Block Of Lafaye...|San Francisco|   CA|        1|Premise Address|                null|\n",
      "|170422249|   Suspicious Person| 2/11/2017|2/11/2017|  2/11/2017|   15:03|2/11/2017 15:03|         GOA|1200 Block Of Arg...|San Francisco|   CA|        1|Common Location|   Kezar Stadium, Sf|\n",
      "|170422353|Assault / Battery Dv| 2/11/2017|2/11/2017|  2/11/2017|   15:32|2/11/2017 15:32|         NOM|Duboce Av/belcher St|San Francisco|   CA|        1|   Intersection|                null|\n",
      "|170422541|    Well Being Check| 2/11/2017|2/11/2017|  2/11/2017|   16:21|2/11/2017 16:21|         HAN|5600 Block Of 3rd St|San Francisco|   CA|        1|Premise Address|                null|\n",
      "|170912461|  Homeless Complaint|  4/1/2017| 4/1/2017|   4/1/2017|   15:10| 4/1/2017 15:10|         GOA|400 Block Of Broa...|San Francisco|   CA|        1|Premise Address|                null|\n",
      "|170912640|  Suspicious Vehicle|  4/1/2017| 4/1/2017|   4/1/2017|   16:00| 4/1/2017 16:00|         ADV|Oakdale Av/mendel...|San Francisco|   CA|        1|   Intersection|                null|\n",
      "|170912646|  Suspicious Vehicle|  4/1/2017| 4/1/2017|   4/1/2017|   16:02| 4/1/2017 16:02|         ADV|1600 Block Of Oak...|San Francisco|   CA|        1|Premise Address|                null|\n",
      "|170912711|  Homeless Complaint|  4/1/2017| 4/1/2017|   4/1/2017|   16:14| 4/1/2017 16:14|         GOA|17th St/san Bruno Av|San Francisco|   CA|        1|   Intersection|                null|\n",
      "|170912736|                  Dw|  4/1/2017| 4/1/2017|   4/1/2017|   16:20| 4/1/2017 16:20|         CAN|700 Block Of Nort...|San Francisco|   CA|        1|Premise Address|                null|\n",
      "|170442017|           Open Line| 2/13/2017|2/13/2017|  2/13/2017|   13:45|2/13/2017 13:45|         NCR|100 Block Of Clay St|San Francisco|   CA|        1|Premise Address|                null|\n",
      "+---------+--------------------+----------+---------+-----------+--------+---------------+------------+--------------------+-------------+-----+---------+---------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n",
      "|Code|  DispositionMeaning| _c2| _c3| _c4| _c5| _c6| _c7| _c8| _c9|_c10|_c11|_c12|_c13|_c14|_c15|\n",
      "+----+--------------------+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n",
      "| ABA|             Abated |null|null|null|null|null|null|null|null|null|null|null|null|null|null|\n",
      "| ADM|          Admonished|null|null|null|null|null|null|null|null|null|null|null|null|null|null|\n",
      "| ADV|             Advised|null|null|null|null|null|null|null|null|null|null|null|null|null|null|\n",
      "| ARR|             Arrest |null|null|null|null|null|null|null|null|null|null|null|null|null|null|\n",
      "| CAN|              Cancel|null|null|null|null|null|null|null|null|null|null|null|null|null|null|\n",
      "| CSA|    CPSA assignment |null|null|null|null|null|null|null|null|null|null|null|null|null|null|\n",
      "|  22|              Cancel|null|null|null|null|null|null|null|null|null|null|null|null|null|null|\n",
      "| CIT|               Cited|null|null|null|null|null|null|null|null|null|null|null|null|null|null|\n",
      "| CRM|Criminal Activation |null|null|null|null|null|null|null|null|null|null|null|null|null|null|\n",
      "| GOA|     Gone on Arrival|null|null|null|null|null|null|null|null|null|null|null|null|null|null|\n",
      "| HAN|            Handled |null|null|null|null|null|null|null|null|null|null|null|null|null|null|\n",
      "| NCR|       Non-Criminal |null|null|null|null|null|null|null|null|null|null|null|null|null|null|\n",
      "|  ND|      No Disposition|null|null|null|null|null|null|null|null|null|null|null|null|null|null|\n",
      "| NOM|           No Merit |null|null|null|null|null|null|null|null|null|null|null|null|null|null|\n",
      "| PAS|Premises Appears ...|null|null|null|null|null|null|null|null|null|null|null|null|null|null|\n",
      "| REP|             Report |null|null|null|null|null|null|null|null|null|null|null|null|null|null|\n",
      "| SFD|SFFD Medical Staf...|null|null|null|null|null|null|null|null|null|null|null|null|null|null|\n",
      "| UTL|    Unable to Locate|null|null|null|null|null|null|null|null|null|null|null|null|null|null|\n",
      "| VAS|Vehicle Appears S...|null|null|null|null|null|null|null|null|null|null|null|null|null|null|\n",
      "+----+--------------------+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dff.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+----------+----+-----+---------------+\n",
      "|IsDrug|DispositionMeaning|DayOfMonth|Hour|Month|    AddressType|\n",
      "+------+------------------+----------+----+-----+---------------+\n",
      "|     0|           Handled|         1|  17|    3|   Intersection|\n",
      "|     0|             Cited|        14|  23|    2|   Intersection|\n",
      "|     0|             Cited|        14|  23|    2|   Intersection|\n",
      "|     0|           Handled|         1|  13|    4|Common Location|\n",
      "|     0|   Gone on Arrival|         9|  20|    2|Common Location|\n",
      "|     0|           Handled|         1|  15|    4|Premise Address|\n",
      "|     0|            Report|        31|  17|    1|Premise Address|\n",
      "|     0|            Report|        19|  11|    2|Premise Address|\n",
      "|     0|           Handled|        21|   7|    3|Premise Address|\n",
      "|     0|           Handled|        23|   7|    3|Premise Address|\n",
      "|     0|   Gone on Arrival|        11|  15|    2|Common Location|\n",
      "|     0|          No Merit|        11|  15|    2|   Intersection|\n",
      "|     0|           Handled|        11|  16|    2|Premise Address|\n",
      "|     0|   Gone on Arrival|         1|  15|    4|Premise Address|\n",
      "|     0|           Advised|         1|  16|    4|   Intersection|\n",
      "|     0|           Advised|         1|  16|    4|Premise Address|\n",
      "|     0|   Gone on Arrival|         1|  16|    4|   Intersection|\n",
      "|     0|            Cancel|         1|  16|    4|Premise Address|\n",
      "|     0|      Non-Criminal|        13|  13|    2|Premise Address|\n",
      "|     0|           Advised|         4|   8|    1|Premise Address|\n",
      "+------+------------------+----------+----+-----+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Crime Id: integer (nullable = true)\n",
      " |-- Original Crime Type Name: string (nullable = true)\n",
      " |-- Report Date: string (nullable = true)\n",
      " |-- Call Date: string (nullable = true)\n",
      " |-- Offense Date: string (nullable = true)\n",
      " |-- Call Time: string (nullable = true)\n",
      " |-- Call Date Time: string (nullable = true)\n",
      " |-- Disposition: string (nullable = true)\n",
      " |-- Address: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Agency Id: integer (nullable = true)\n",
      " |-- Address Type: string (nullable = true)\n",
      " |-- Common Location: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|         City|\n",
      "+-------------+\n",
      "|   Fort Mason|\n",
      "|San Francisco|\n",
      "|         null|\n",
      "|     Brisbane|\n",
      "|    Daly City|\n",
      "|  Yerba Buena|\n",
      "|     Presidio|\n",
      "|Hunters Point|\n",
      "|Treasure Isla|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"City\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|State|\n",
      "+-----+\n",
      "|   CA|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"State\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|   Address Type|\n",
      "+---------------+\n",
      "|   Intersection|\n",
      "|Common Location|\n",
      "|Premise Address|\n",
      "|   Geo-Override|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"Address Type\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|Disposition|\n",
      "+-----------+\n",
      "|        CHP|\n",
      "|        DUP|\n",
      "|        ABA|\n",
      "|        INC|\n",
      "|        TH2|\n",
      "|         22|\n",
      "|        NOM|\n",
      "|        HOT|\n",
      "|        GOA|\n",
      "|        CIT|\n",
      "|        HAN|\n",
      "|        ADM|\n",
      "|        ENC|\n",
      "|        FIR|\n",
      "|        OME|\n",
      "|         ND|\n",
      "|        SFD|\n",
      "|        PAS|\n",
      "|        CRT|\n",
      "|        UTL|\n",
      "+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"Disposition\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"SanFran\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|Month|Count|\n",
      "+-----+-----+\n",
      "|    9|62518|\n",
      "|    8|56213|\n",
      "|    3|55649|\n",
      "|    1|51322|\n",
      "|    5|49950|\n",
      "|    2|47564|\n",
      "|    4|46407|\n",
      "|    7|44662|\n",
      "|    6|42713|\n",
      "|   10|23856|\n",
      "|   11|  102|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfQuery = spark.sql(\"SELECT month(CAST(UNIX_TIMESTAMP(CallDate, 'MM/dd/yyyy') AS TIMESTAMP)) AS Month, COUNT(*) AS Count FROM SanFran GROUP BY month(CAST(UNIX_TIMESTAMP(CallDate, 'MM/dd/yyyy') AS TIMESTAMP)) ORDER BY Count DESC\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+----------+--------+-----------+--------+-------------+-----------+--------------------+-----------+------+---------+---------------+--------------------+\n",
      "|summary|             CrimeId|OriginalCrimeTypName|ReportDate|CallDate|OffenseDate|CallTime| CallDateTime|Disposition|             Address|       City| State|Agency Id|    AddressType|      CommonLocation|\n",
      "+-------+--------------------+--------------------+----------+--------+-----------+--------+-------------+-----------+--------------------+-----------+------+---------+---------------+--------------------+\n",
      "|  count|              480956|              480956|    480956|  480956|     480956|  480956|       480956|     480956|              480956|     467364|480956|   480956|         480956|               48954|\n",
      "|   mean|1.7146812263684413E8|   8837.803080576368|      null|    null|       null|    null|         null|       22.0|   734.3333333333334|       null|  null|      1.0|           null|                null|\n",
      "| stddev|   851963.8276113913|   364435.0293523288|      null|    null|       null|    null|         null|        0.0|  152.07344716725973|       null|  null|      0.0|           null|                null|\n",
      "|    min|           170010001|         \"18\"\" Pipe\"|  1/1/2017|1/1/2017|   1/1/2017|    0:00|1/1/2017 0:00|         22|\"2122 Taraval St ...|   Brisbane|    CA|        1|Common Location|1 Market Pz #Lvl ...|\n",
      "|    max|           173192201|              `Drugs|  9/9/2017|9/9/2017|   9/9/2017|    9:59|9/9/2017 9:59|        VAS|    `Mmonterey/congo|Yerba Buena|    CA|        1|Premise Address|  Zoo-South Gate, Sf|\n",
      "+-------+--------------------+--------------------+----------+--------+-----------+--------+-------------+-----------+--------------------+-----------+------+---------+---------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------+\n",
      "|         City| Count|\n",
      "+-------------+------+\n",
      "|   Fort Mason|    11|\n",
      "|San Francisco|464769|\n",
      "|         null| 13592|\n",
      "|     Brisbane|     8|\n",
      "|    Daly City|   258|\n",
      "|  Yerba Buena|   137|\n",
      "|     Presidio|   133|\n",
      "|Hunters Point|   144|\n",
      "|Treasure Isla|  1904|\n",
      "+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfQuery = spark.sql(\"SELECT DISTINCT City, COUNT(*) AS Count FROM SanFran GROUP BY City\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|COUNT|\n",
      "+-----+\n",
      "| 6408|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfQuery = spark.sql(\"SELECT COUNT(DISTINCT OriginalCrimeTypName) AS COUNT FROM SanFran\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------+------------+---------------+\n",
      "|OriginalCrimeTypName|ReportDate|CallTime| Disposition|    AddressType|\n",
      "+--------------------+----------+--------+------------+---------------+\n",
      "|        Passing Call| 2/12/2017|   17:04|Not recorded|   Intersection|\n",
      "|     Muni Inspection|  3/1/2017|   17:37|         HAN|   Intersection|\n",
      "|        Traffic Stop| 2/14/2017|   23:01|         CIT|   Intersection|\n",
      "|        Traffic Stop| 2/14/2017|   23:01|         CIT|   Intersection|\n",
      "|        Passing Call|  4/1/2017|   13:46|         HAN|Common Location|\n",
      "|         Return Call|  2/9/2017|   20:40|         GOA|Common Location|\n",
      "|          Trespasser|  4/1/2017|   15:06|         HAN|Premise Address|\n",
      "|         Petty Theft| 1/31/2017|   17:39|         REP|Premise Address|\n",
      "|                 Rep| 2/19/2017|   11:20|         REP|Premise Address|\n",
      "|                 915| 3/21/2017|    7:33|         HAN|Premise Address|\n",
      "|              22500e| 3/23/2017|    7:57|         HAN|Premise Address|\n",
      "|   Suspicious Person| 2/11/2017|   15:03|         GOA|Common Location|\n",
      "|Assault / Battery Dv| 2/11/2017|   15:32|         NOM|   Intersection|\n",
      "|    Well Being Check| 2/11/2017|   16:21|         HAN|Premise Address|\n",
      "|  Homeless Complaint|  4/1/2017|   15:10|         GOA|Premise Address|\n",
      "|  Suspicious Vehicle|  4/1/2017|   16:00|         ADV|   Intersection|\n",
      "|  Suspicious Vehicle|  4/1/2017|   16:02|         ADV|Premise Address|\n",
      "|  Homeless Complaint|  4/1/2017|   16:14|         GOA|   Intersection|\n",
      "|                  Dw|  4/1/2017|   16:20|         CAN|Premise Address|\n",
      "|           Open Line| 2/13/2017|   13:45|         NCR|Premise Address|\n",
      "+--------------------+----------+--------+------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfQuery = spark.sql(\"SELECT OriginalCrimeTypName, ReportDate, CallTime, Disposition, AddressType FROM SanFran\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------+------------+---------------+\n",
      "|OriginalCrimeTypName|ReportDate|CallTime| Disposition|    AddressType|\n",
      "+--------------------+----------+--------+------------+---------------+\n",
      "|        Passing Call| 2/12/2017|   17:04|Not recorded|   Intersection|\n",
      "|     Muni Inspection|  3/1/2017|   17:37|         HAN|   Intersection|\n",
      "|        Traffic Stop| 2/14/2017|   23:01|         CIT|   Intersection|\n",
      "|        Traffic Stop| 2/14/2017|   23:01|         CIT|   Intersection|\n",
      "|        Passing Call|  4/1/2017|   13:46|         HAN|Common Location|\n",
      "|         Return Call|  2/9/2017|   20:40|         GOA|Common Location|\n",
      "|          Trespasser|  4/1/2017|   15:06|         HAN|Premise Address|\n",
      "|         Petty Theft| 1/31/2017|   17:39|         REP|Premise Address|\n",
      "|                 Rep| 2/19/2017|   11:20|         REP|Premise Address|\n",
      "|                 915| 3/21/2017|    7:33|         HAN|Premise Address|\n",
      "|              22500e| 3/23/2017|    7:57|         HAN|Premise Address|\n",
      "|   Suspicious Person| 2/11/2017|   15:03|         GOA|Common Location|\n",
      "|Assault / Battery Dv| 2/11/2017|   15:32|         NOM|   Intersection|\n",
      "|    Well Being Check| 2/11/2017|   16:21|         HAN|Premise Address|\n",
      "|  Homeless Complaint|  4/1/2017|   15:10|         GOA|Premise Address|\n",
      "|  Suspicious Vehicle|  4/1/2017|   16:00|         ADV|   Intersection|\n",
      "|  Suspicious Vehicle|  4/1/2017|   16:02|         ADV|Premise Address|\n",
      "|  Homeless Complaint|  4/1/2017|   16:14|         GOA|   Intersection|\n",
      "|                  Dw|  4/1/2017|   16:20|         CAN|Premise Address|\n",
      "|           Open Line| 2/13/2017|   13:45|         NCR|Premise Address|\n",
      "+--------------------+----------+--------+------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfQuery = spark.sql(\"SELECT OriginalCrimeTypName, ReportDate, CallTime, Disposition, AddressType \n",
    "                    FROM SanFran \n",
    "                    WHERE City = 'San Francisco'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|IsDrug|\n",
      "+------+\n",
      "|     0|\n",
      "|     0|\n",
      "|     0|\n",
      "|     0|\n",
      "|     0|\n",
      "|     0|\n",
      "|     0|\n",
      "|     0|\n",
      "|     0|\n",
      "|     0|\n",
      "|     0|\n",
      "|     0|\n",
      "|     0|\n",
      "|     0|\n",
      "|     0|\n",
      "|     0|\n",
      "|     0|\n",
      "|     0|\n",
      "|     0|\n",
      "|     0|\n",
      "+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfQuery = spark.sql(\"SELECT CASE WHEN OriginalCrimeTypName LIKE '%Drug%' THEN 1 ELSE 0 END AS IsDrug \n",
    "                    FROM SanFran \n",
    "                    WHERE City = 'San Francisco'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+-----+---------------+\n",
      "|DayOfMonth|Hour|Month|    AddressType|\n",
      "+----------+----+-----+---------------+\n",
      "|        12|  17|    2|   Intersection|\n",
      "|         1|  17|    3|   Intersection|\n",
      "|        14|  23|    2|   Intersection|\n",
      "|        14|  23|    2|   Intersection|\n",
      "|         1|  13|    4|Common Location|\n",
      "|         9|  20|    2|Common Location|\n",
      "|         1|  15|    4|Premise Address|\n",
      "|        31|  17|    1|Premise Address|\n",
      "|        19|  11|    2|Premise Address|\n",
      "|        21|   7|    3|Premise Address|\n",
      "|        23|   7|    3|Premise Address|\n",
      "|        11|  15|    2|Common Location|\n",
      "|        11|  15|    2|   Intersection|\n",
      "|        11|  16|    2|Premise Address|\n",
      "|         1|  15|    4|Premise Address|\n",
      "|         1|  16|    4|   Intersection|\n",
      "|         1|  16|    4|Premise Address|\n",
      "|         1|  16|    4|   Intersection|\n",
      "|         1|  16|    4|Premise Address|\n",
      "|        13|  13|    2|Premise Address|\n",
      "+----------+----+-----+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfQuery = spark.sql(\"SELECT dayofmonth(CAST(UNIX_TIMESTAMP(CallDate, 'MM/dd/yyyy') AS TIMESTAMP)) AS DayOfMonth, hour(CallTime) AS Hour, month(CAST(UNIX_TIMESTAMP(CallDate, 'MM/dd/yyyy') AS TIMESTAMP)) AS Month, AddressType FROM SanFran\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff.createOrReplaceTempView(\"Disposition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "CASE WHEN SanFran.OriginalCrimeTypName LIKE '%Drug%' THEN 1 ELSE 0 END AS IsDrug, dayofmonth(CAST(UNIX_TIMESTAMP(SanFran.CallDate, 'MM/dd/yyyy') AS TIMESTAMP)) AS DayOfMonth, hour(SanFran.CallTime) AS Hour, month(CAST(UNIX_TIMESTAMP(SanFran.CallDate, 'MM/dd/yyyy') AS TIMESTAMP)) AS Month, \n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+----------+----+-----+---------------+\n",
      "|IsDrug|DispositionMeaning|DayOfMonth|Hour|Month|    AddressType|\n",
      "+------+------------------+----------+----+-----+---------------+\n",
      "|     0|           Handled|         1|  17|    3|   Intersection|\n",
      "|     0|             Cited|        14|  23|    2|   Intersection|\n",
      "|     0|             Cited|        14|  23|    2|   Intersection|\n",
      "|     0|           Handled|         1|  13|    4|Common Location|\n",
      "|     0|   Gone on Arrival|         9|  20|    2|Common Location|\n",
      "|     0|           Handled|         1|  15|    4|Premise Address|\n",
      "|     0|            Report|        31|  17|    1|Premise Address|\n",
      "|     0|            Report|        19|  11|    2|Premise Address|\n",
      "|     0|           Handled|        21|   7|    3|Premise Address|\n",
      "|     0|           Handled|        23|   7|    3|Premise Address|\n",
      "|     0|   Gone on Arrival|        11|  15|    2|Common Location|\n",
      "|     0|          No Merit|        11|  15|    2|   Intersection|\n",
      "|     0|           Handled|        11|  16|    2|Premise Address|\n",
      "|     0|   Gone on Arrival|         1|  15|    4|Premise Address|\n",
      "|     0|           Advised|         1|  16|    4|   Intersection|\n",
      "|     0|           Advised|         1|  16|    4|Premise Address|\n",
      "|     0|   Gone on Arrival|         1|  16|    4|   Intersection|\n",
      "|     0|            Cancel|         1|  16|    4|Premise Address|\n",
      "|     0|      Non-Criminal|        13|  13|    2|Premise Address|\n",
      "|     0|           Advised|         4|   8|    1|Premise Address|\n",
      "+------+------------------+----------+----+-----+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and Justify test Designs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 299749\n",
      "Test Dataset Count: 128402\n"
     ]
    }
   ],
   "source": [
    "(trainingData, testData) = final.randomSplit([0.7, 0.3], seed = 100)\n",
    "print(\"Training Dataset Count: \" + str(trainingData.count()))\n",
    "print(\"Test Dataset Count: \" + str(testData.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "lrModel = lr.fit(trainingData)\n",
    "predictions = lrModel.transform(testData)\n",
    "predictions.filter(predictions['IsDrug'] == 0) \\\n",
    "    .select(\"DispositionMeaning\",\"DayOfMonth\",\"Hour\",\"Month\",\"AddressType\", \"IsDrug\") \\\n",
    "    .orderBy(\"IsDrug\", ascending=False) \\\n",
    "    .show(n = 10, truncate = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "'Field \"features\" does not exist.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o329.fit.\n: java.lang.IllegalArgumentException: Field \"features\" does not exist.\n\tat org.apache.spark.sql.types.StructType$$anonfun$apply$1.apply(StructType.scala:264)\n\tat org.apache.spark.sql.types.StructType$$anonfun$apply$1.apply(StructType.scala:264)\n\tat scala.collection.MapLike$class.getOrElse(MapLike.scala:128)\n\tat scala.collection.AbstractMap.getOrElse(Map.scala:59)\n\tat org.apache.spark.sql.types.StructType.apply(StructType.scala:263)\n\tat org.apache.spark.ml.util.SchemaUtils$.checkColumnType(SchemaUtils.scala:40)\n\tat org.apache.spark.ml.PredictorParams$class.validateAndTransformSchema(Predictor.scala:51)\n\tat org.apache.spark.ml.classification.Classifier.org$apache$spark$ml$classification$ClassifierParams$$super$validateAndTransformSchema(Classifier.scala:58)\n\tat org.apache.spark.ml.classification.ClassifierParams$class.validateAndTransformSchema(Classifier.scala:42)\n\tat org.apache.spark.ml.classification.ProbabilisticClassifier.org$apache$spark$ml$classification$ProbabilisticClassifierParams$$super$validateAndTransformSchema(ProbabilisticClassifier.scala:53)\n\tat org.apache.spark.ml.classification.ProbabilisticClassifierParams$class.validateAndTransformSchema(ProbabilisticClassifier.scala:37)\n\tat org.apache.spark.ml.classification.ProbabilisticClassifier.validateAndTransformSchema(ProbabilisticClassifier.scala:53)\n\tat org.apache.spark.ml.Predictor.transformSchema(Predictor.scala:122)\n\tat org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:74)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:90)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:72)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-8a1e55a9b639>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mrf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabelCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m,\u001b[0m                             \u001b[0mfeaturesCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"features\"\u001b[0m\u001b[0;34m,\u001b[0m                             \u001b[0mnumTrees\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m                             \u001b[0mmaxDepth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m                             \u001b[0mmaxBins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Train model with Training Data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mrfModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainingData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrfModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'IsDrug'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"DispositionMeaning\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"DayOfMonth\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Hour\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Month\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"AddressType\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"IsDrug\"\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0morderBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"probability\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    231\u001b[0m         \"\"\"\n\u001b[1;32m    232\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: 'Field \"features\" does not exist.'"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "rf = RandomForestClassifier(labelCol=\"label\", \\\n",
    "                            featuresCol=\"features\", \\\n",
    "                            numTrees = 100, \\\n",
    "                            maxDepth = 4, \\\n",
    "                            maxBins = 32)\n",
    "# Train model with Training Data\n",
    "rfModel = rf.fit(trainingData)\n",
    "predictions = rfModel.transform(testData)\n",
    "predictions.filter(predictions['IsDrug'] == 0) \\\n",
    "    .select(\"DispositionMeaning\",\"DayOfMonth\",\"Hour\",\"Month\",\"AddressType\", \"IsDrug\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 10, truncate = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
